# AI
## llama.cpp
### demo简介
#### demo说明
用于 C/C++ 的 LLM 推理框架

#### demo源码链接
https://github.com/ggerganov/llama.cpp

#### sdk及链接
https://github.com/ggerganov/llama.cpp

#### 环境说明
硬件：Milkv Jupiter

系统：Bianbu 2.0 (GNU/Linux 6.6.36 riscv64)

### Demo运行
[llama.cpp](llama.cpp.md)

### sdk集成说明
需要考虑使用GPU、NPU等进行加速，否则推理速度极慢

### Demo运行总结
在低算力低内存占用的前提下运行大模型推理，很有意思

但是CPU推理速度太慢，推理加速的适配还不是很好
